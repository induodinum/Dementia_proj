{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/admin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  \n",
    "from sklearn import metrics\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_each(x):\n",
    "    std = np.std(x, axis=0 , ddof= 1)\n",
    "    std = math.pow(std,2)\n",
    "    std = np.array(std)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI(arr):\n",
    "    trisum = 0\n",
    "    result = []\n",
    "    for i in range(len(arr[0])):\n",
    "        trisum += arr[0][i][0]\n",
    "        trisum += arr[0][i][1]\n",
    "        trisum += arr[0][i][2]\n",
    "        \n",
    "        result.append(math.sqrt(max(trisum/3,0)))\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest(arr):\n",
    "    std = np.std(arr, axis=0 , ddof= 1)\n",
    "    std = math.pow(std,2)\n",
    "    std = np.array(std)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"STANDING\"\n",
    "]\n",
    "\n",
    "LABELS_REST = [\n",
    "    \"SITTING\",\n",
    "    \"LAYING\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    X_signals_rest = []\n",
    "\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "      \"UCI HAR Dataset/train/\" + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    \"UCI HAR Dataset/test/\" + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ - 1\n",
    "\n",
    "y_train_path = \"UCI HAR Dataset/train/y_train.txt\"\n",
    "y_test_path = \"UCI HAR Dataset/test/y_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Active and Inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(X_train)):\n",
    "#     if(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "-------------\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(\"-------------\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.041216   -0.2697959   0.02377977]\n",
      "  [ 1.041803   -0.280025    0.07629271]\n",
      "  [ 1.039086   -0.2926631   0.1474754 ]\n",
      "  ...\n",
      "  [ 0.9930164  -0.2599865   0.1443951 ]\n",
      "  [ 0.9932414  -0.2620643   0.1447033 ]\n",
      "  [ 0.9943906  -0.2641348   0.1454939 ]]\n",
      "\n",
      " [[ 0.9991921  -0.2649349   0.1256164 ]\n",
      "  [ 0.9946787  -0.2532142   0.1256249 ]\n",
      "  [ 0.9935518  -0.2565887   0.1163814 ]\n",
      "  ...\n",
      "  [ 1.001861   -0.2619359   0.1527878 ]\n",
      "  [ 0.9975208  -0.2713225   0.1398428 ]\n",
      "  [ 0.9928615  -0.2799715   0.1213135 ]]\n",
      "\n",
      " [[ 0.9975931  -0.2639912   0.1507741 ]\n",
      "  [ 0.9989703  -0.2638194   0.1539427 ]\n",
      "  [ 0.9970574  -0.2638495   0.1441536 ]\n",
      "  ...\n",
      "  [ 0.9918802  -0.2836712   0.132678  ]\n",
      "  [ 0.9906626  -0.280597    0.1326941 ]\n",
      "  [ 0.9882446  -0.2822329   0.1321175 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.8213505  -0.2484623  -0.2216934 ]\n",
      "  [ 0.7991996  -0.2232599  -0.2045561 ]\n",
      "  [ 0.8004623  -0.179017   -0.2568719 ]\n",
      "  ...\n",
      "  [ 1.46317    -0.5515283  -0.2723974 ]\n",
      "  [ 1.179223   -0.5472997  -0.06773376]\n",
      "  [ 0.8504963  -0.4900368   0.1378256 ]]\n",
      "\n",
      " [[ 1.037668   -0.3971532  -0.3940817 ]\n",
      "  [ 0.8780725  -0.2848634  -0.3151097 ]\n",
      "  [ 0.8963897  -0.2635297  -0.213904  ]\n",
      "  ...\n",
      "  [ 1.156389   -0.2283478  -0.00351205]\n",
      "  [ 1.243857   -0.258322   -0.1117857 ]\n",
      "  [ 1.323546   -0.3472416  -0.2760682 ]]\n",
      "\n",
      " [[ 0.7713622  -0.4250499  -0.05327655]\n",
      "  [ 0.9000949  -0.4375916  -0.4020727 ]\n",
      "  [ 0.8681034  -0.4421595  -0.5197379 ]\n",
      "  ...\n",
      "  [ 0.9188616  -0.3516799  -0.07253919]\n",
      "  [ 0.9494752  -0.267526   -0.05097549]\n",
      "  [ 0.9578348  -0.1941603  -0.02892477]]]\n",
      "-------------\n",
      "-----------------------------------------------------------\n",
      "[[ 1.041216   -0.2697959   0.02377977]\n",
      " [ 1.041803   -0.280025    0.07629271]\n",
      " [ 1.039086   -0.2926631   0.1474754 ]\n",
      " [ 1.054768   -0.2923837   0.1399061 ]\n",
      " [ 1.028376   -0.2858257   0.1199341 ]\n",
      " [ 1.006216   -0.2782526   0.1341948 ]\n",
      " [ 0.9623905  -0.2587233   0.1333636 ]\n",
      " [ 0.9291962  -0.2494186   0.142489  ]\n",
      " [ 0.9430467  -0.2629814   0.1609307 ]\n",
      " [ 0.9516348  -0.2628907   0.1416763 ]\n",
      " [ 1.001703   -0.257446    0.1225485 ]\n",
      " [ 1.017726   -0.2688286   0.1261423 ]\n",
      " [ 0.98902    -0.280794    0.1392838 ]\n",
      " [ 1.006362   -0.2923927   0.1592329 ]\n",
      " [ 1.001875   -0.284969    0.1602542 ]\n",
      " [ 0.9999006  -0.2579842   0.1523619 ]\n",
      " [ 1.009093   -0.2544064   0.1515498 ]\n",
      " [ 0.9914053  -0.261836    0.1429909 ]\n",
      " [ 0.9964305  -0.2588272   0.1405367 ]\n",
      " [ 0.9920888  -0.2551794   0.1471643 ]\n",
      " [ 0.9797703  -0.2531704   0.1484713 ]\n",
      " [ 0.9891511  -0.2633458   0.1475489 ]\n",
      " [ 1.000643   -0.2705073   0.1398946 ]\n",
      " [ 1.006941   -0.25251     0.1355136 ]\n",
      " [ 0.9927965  -0.2393442   0.1488608 ]\n",
      " [ 0.9880946  -0.2582156   0.1567169 ]\n",
      " [ 0.9802254  -0.2755857   0.1514509 ]\n",
      " [ 0.9520292  -0.2592684   0.1481351 ]\n",
      " [ 0.9488618  -0.2449041   0.1398829 ]\n",
      " [ 0.9585349  -0.2447039   0.1248992 ]\n",
      " [ 0.976567   -0.2400349   0.1189882 ]\n",
      " [ 0.9870492  -0.2578569   0.1304884 ]\n",
      " [ 0.9961715  -0.281942    0.1382731 ]\n",
      " [ 1.027648   -0.2775408   0.1214311 ]\n",
      " [ 1.026131   -0.2776241   0.1079785 ]\n",
      " [ 1.022794   -0.2881434   0.1172064 ]\n",
      " [ 1.010605   -0.2806487   0.1232793 ]\n",
      " [ 0.9651603  -0.2678075   0.1195766 ]\n",
      " [ 0.9889361  -0.2623667   0.1209272 ]\n",
      " [ 1.022747   -0.2549691   0.1226699 ]\n",
      " [ 1.013427   -0.2538533   0.1214685 ]\n",
      " [ 1.019843   -0.2595405   0.1233823 ]\n",
      " [ 1.002479   -0.2656444   0.1252134 ]\n",
      " [ 0.9838019  -0.277214    0.1291742 ]\n",
      " [ 0.9883876  -0.2776479   0.1343704 ]\n",
      " [ 0.9831198  -0.2713087   0.1348222 ]\n",
      " [ 0.9855486  -0.2748284   0.1339379 ]\n",
      " [ 0.9912714  -0.2729923   0.1275461 ]\n",
      " [ 0.9993174  -0.2749015   0.1187621 ]\n",
      " [ 1.002052   -0.2825422   0.1165818 ]\n",
      " [ 0.9931758  -0.2772074   0.1168493 ]\n",
      " [ 0.9924299  -0.2708342   0.1194555 ]\n",
      " [ 0.9879958  -0.2675208   0.1258951 ]\n",
      " [ 1.001712   -0.2671768   0.1349576 ]\n",
      " [ 1.025348   -0.2715603   0.1363237 ]\n",
      " [ 1.011759   -0.2680104   0.1168667 ]\n",
      " [ 0.9917118  -0.2630059   0.09368241]\n",
      " [ 0.9742868  -0.2609582   0.09459222]\n",
      " [ 0.9612096  -0.259832    0.112969  ]\n",
      " [ 0.9814763  -0.2653533   0.122927  ]\n",
      " [ 0.9958289  -0.269758    0.1226852 ]\n",
      " [ 0.987968   -0.2734924   0.1238651 ]\n",
      " [ 0.9934975  -0.2765396   0.1267587 ]\n",
      " [ 1.002419   -0.2727638   0.1267468 ]\n",
      " [ 0.9991921  -0.2649349   0.1256164 ]\n",
      " [ 0.9946787  -0.2532142   0.1256249 ]\n",
      " [ 0.9935518  -0.2565887   0.1163814 ]\n",
      " [ 0.9986743  -0.2729519   0.1075668 ]\n",
      " [ 1.00549    -0.2759762   0.115386  ]\n",
      " [ 1.0033     -0.274949    0.1236646 ]\n",
      " [ 1.002225   -0.273747    0.1256672 ]\n",
      " [ 1.010279   -0.2758725   0.1238663 ]\n",
      " [ 1.01578    -0.2818627   0.1221028 ]\n",
      " [ 1.019949   -0.2741971   0.1312971 ]\n",
      " [ 1.026733   -0.266373    0.1403063 ]\n",
      " [ 1.015277   -0.2612068   0.1436541 ]\n",
      " [ 0.9673556  -0.2568455   0.156707  ]\n",
      " [ 0.9282254  -0.2626477   0.1732641 ]\n",
      " [ 0.9640761  -0.2664683   0.1679331 ]\n",
      " [ 1.009091   -0.2729597   0.1558112 ]\n",
      " [ 0.9869095  -0.2765982   0.1611386 ]\n",
      " [ 0.9799691  -0.2711735   0.1601415 ]\n",
      " [ 0.9984525  -0.2732885   0.1577895 ]\n",
      " [ 0.9846282  -0.2716742   0.1671403 ]\n",
      " [ 0.9871465  -0.2654026   0.1660506 ]\n",
      " [ 0.994021   -0.2643399   0.1628516 ]\n",
      " [ 0.9837897  -0.2661548   0.1609977 ]\n",
      " [ 0.9911906  -0.2714743   0.1587854 ]\n",
      " [ 0.9875496  -0.2690153   0.1604427 ]\n",
      " [ 0.9828639  -0.2659332   0.1515226 ]\n",
      " [ 0.9896043  -0.2699687   0.147972  ]\n",
      " [ 0.9858347  -0.2694207   0.1541673 ]\n",
      " [ 0.9896334  -0.2706686   0.1537571 ]\n",
      " [ 0.9881861  -0.270762    0.1517306 ]\n",
      " [ 0.983746   -0.2660981   0.146116  ]\n",
      " [ 0.9901273  -0.2666628   0.1430204 ]\n",
      " [ 0.9904191  -0.2669799   0.1436079 ]\n",
      " [ 0.9923201  -0.2672199   0.1442734 ]\n",
      " [ 0.9918436  -0.2693626   0.1495746 ]\n",
      " [ 0.9886363  -0.2703799   0.1503135 ]\n",
      " [ 0.9899682  -0.273063    0.1499668 ]\n",
      " [ 0.9896729  -0.2729013   0.1504297 ]\n",
      " [ 0.9929223  -0.2722157   0.148942  ]\n",
      " [ 0.9930339  -0.2741134   0.1499768 ]\n",
      " [ 0.9920592  -0.2718528   0.1479618 ]\n",
      " [ 0.9943507  -0.2699831   0.1483899 ]\n",
      " [ 0.9922506  -0.2723409   0.1508469 ]\n",
      " [ 0.9936145  -0.274476    0.1507295 ]\n",
      " [ 0.9963205  -0.2752326   0.152542  ]\n",
      " [ 0.9970623  -0.2733163   0.15145   ]\n",
      " [ 0.9960405  -0.2677135   0.1537508 ]\n",
      " [ 0.9884157  -0.2605185   0.1586551 ]\n",
      " [ 0.9846389  -0.2562781   0.1580381 ]\n",
      " [ 0.9885686  -0.2568547   0.1587177 ]\n",
      " [ 0.9935828  -0.2582192   0.1587117 ]\n",
      " [ 0.9956121  -0.2583914   0.1596097 ]\n",
      " [ 0.9933585  -0.2581007   0.1618209 ]\n",
      " [ 0.990748   -0.2564464   0.1581316 ]\n",
      " [ 0.9884197  -0.255866    0.1551095 ]\n",
      " [ 0.9901729  -0.2601251   0.1533461 ]\n",
      " [ 0.9943224  -0.2655684   0.1492895 ]\n",
      " [ 0.9938816  -0.2674106   0.1474012 ]\n",
      " [ 0.9942994  -0.2687901   0.1469048 ]\n",
      " [ 0.9948837  -0.2695789   0.1452615 ]\n",
      " [ 0.9929783  -0.2644958   0.1439041 ]\n",
      " [ 0.9930164  -0.2599865   0.1443951 ]\n",
      " [ 0.9932414  -0.2620643   0.1447033 ]\n",
      " [ 0.9943906  -0.2641348   0.1454939 ]]\n",
      "-----------------------------------------------------------\n",
      "[ 1.041216   -0.2697959   0.02377977]\n",
      "-----------------------------------------------------------\n",
      "1.012817\n",
      "-----------------------------------------------------------\n",
      "7352\n",
      "-----------------------------------------------------------\n",
      "128\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(\"-------------\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(X_test[0])\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(X_test[0][0])\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(X_train[0][0][0])\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(len(X_train))\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import more_itertools as mit\n",
    "###print(list(mit.windowed(X_train, n=64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equation_bi(arr):\n",
    "    ai = []\n",
    "    result = []\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range (len(arr)):\n",
    "        tri = []\n",
    "        for j in range(len(arr[i])):\n",
    "            \n",
    "            x.append(arr[i][j][0])\n",
    "            y.append(arr[i][j][1])\n",
    "            z.append(arr[i][j][2])\n",
    "        \n",
    "        \n",
    "        tri.append(sigma_each(x))\n",
    "        tri.append(sigma_each(y))\n",
    "        tri.append(sigma_each(z))\n",
    "        tri = np.array(tri)\n",
    "        ai.append(tri)\n",
    "\n",
    "    result.append(ai)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = equation_bi(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train[0]))\n",
    "print(len(X_train[0][1]))\n",
    "print((X_train[0][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = AI(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0],X_train[1],X_train[2],X_train[3],X_train[4])\n",
    "print(X_train[5],X_train[6],X_train[7],X_train[8],X_train[9])\n",
    "print(X_train[10],X_train[11],X_train[12],X_train[13],X_train[14])\n",
    "print(X_train[15],X_train[16],X_train[17],X_train[18],X_train[19])\n",
    "print(X_train[20])\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = equation_bi(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)\n",
    "print(len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = AI(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(7352,1,1)\n",
    "X_test = X_test.reshape(2947,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(7352,6)\n",
    "y_test = y_test.reshape(2947,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTMCell , RNN , LSTM , Embedding, SimpleRNN, Input\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = y_train\n",
    "ytest = y_test\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "\n",
    "verbose, epochs, batch_size = 1, 1000, 64\n",
    "n_outputs = y_train.shape[1]\n",
    "\n",
    "cells = [\n",
    "    LSTMCell(output_dim),\n",
    "    LSTMCell(output_dim)\n",
    "]\n",
    "\n",
    "inputs = Input(shape=(1,1))\n",
    "model = LSTMCell(100,activation = 'tanh',use_bias = True,unit_forget_bias=True)(inputs)\n",
    "model = LSTMCell(100,activation = 'tanh',use_bias = True,unit_forget_bias=True)(model)\n",
    "model = Dense(100 , activation = 'relu')(model)\n",
    "model = Dense(n_outputs , activation = 'softmax')\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(100,unit_forget_bias =1, input_shape=(1,1)))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=0, verbose=1)\n",
    "\t# fit network\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,callbacks=[earlystopper], verbose=verbose)\n",
    "\t# evaluate model\n",
    "_, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy * 100.0\n",
    "summarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label))\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {1:'WALKING', 2:'WALKING UPSTAIRS', 3:'WALKING DOWNSTAIRS',\n",
    "          4:'SITTING', 5:'STANDING', 6:'LAYING'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "n_steps = 1  # 128 timesteps per series\n",
    "n_input = 1  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 100  \n",
    "batch_size = 1500\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "print(X_test.shape, y_test.shape,X_train.shape, np.mean(X_test), np.std(X_test))\n",
    "print(n_steps , n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters.\n",
    "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network.\n",
    "    # Note, some code of this notebook is inspired from an slightly different\n",
    "    # RNN architecture used on another dataset, some of the credits goes to\n",
    "    # \"aymericdamien\" under the MIT license.\n",
    "\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input])\n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "    # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many-to-one\" style classifier\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "def extract_batch_size(_train,step,batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.\n",
    "\n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index]\n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_, n_classes=n_classes):\n",
    "    # Function to encode neural one-hot output labels from number indexes\n",
    "    # e.g.:\n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None,n_steps,batch_size])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_batch_size(X_train))\n",
    "print(len(extract_batch_size(X_train)))\n",
    "print(x, \":\",extract_batch_size(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "plt.plot(indep_test_axis, np.array(test_losses),     \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "#print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "#print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "#print(\"\")\n",
    "#print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "#print(normalised_confusion_matrix)\n",
    "#print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "#print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results:\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix,\n",
    "    interpolation='nearest',\n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import squareform\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class KnnDtw(object):\n",
    "    \"\"\"K-nearest neighbor classifier using dynamic time warping\n",
    "    as the distance measure between pairs of time series arrays\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for KNN\n",
    "        \n",
    "    max_warping_window : int, optional (default = infinity)\n",
    "        Maximum warping window allowed by the DTW dynamic\n",
    "        programming function\n",
    "            \n",
    "    subsample_step : int, optional (default = 1)\n",
    "        Step size for the timeseries array. By setting subsample_step = 2,\n",
    "        the timeseries length will be reduced by 50% because every second\n",
    "        item is skipped. Implemented by x[:, ::subsample_step]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, max_warping_window=10000, subsample_step=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_warping_window = max_warping_window\n",
    "        self.subsample_step = subsample_step\n",
    "    \n",
    "    def fit(self, x, l):\n",
    "        \"\"\"Fit the model using x as training data and l as class labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "            Training data set for input into KNN classifer\n",
    "            \n",
    "        l : array of shape [n_samples]\n",
    "            Training labels for input into KNN classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.l = l\n",
    "        \n",
    "    def _dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):\n",
    "        \"\"\"Returns the DTW similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "        \n",
    "        d : DistanceMetric object (default = abs(x-y))\n",
    "            the distance measure used for A_i - B_j in the\n",
    "            DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "\n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        ts_a, ts_b = np.array(ts_a), np.array(ts_b)\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "        cost = sys.maxsize * np.ones((M, N))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d(ts_a[0], ts_b[0])\n",
    "        for i in range(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])\n",
    "\n",
    "        for j in range(1, N):\n",
    "            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])\n",
    "\n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in range(1, M):\n",
    "            for j in range(max(1, i - self.max_warping_window),\n",
    "                            min(N, i + self.max_warping_window)):\n",
    "                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])\n",
    "\n",
    "        # Return DTW distance given window \n",
    "        return cost[-1, -1]\n",
    "    \n",
    "    def _dist_matrix(self, x, y):\n",
    "        \"\"\"Computes the M x N distance matrix between the training\n",
    "        dataset and testing dataset (y) using the DTW distance measure\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        y : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Distance matrix between each item of x and y with\n",
    "            shape [training_n_samples, testing_n_samples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the distance matrix        \n",
    "        dm_count = 0\n",
    "        \n",
    "        # Compute condensed distance matrix (upper triangle) of pairwise dtw distances\n",
    "        # when x and y are the same array\n",
    "        if(np.array_equal(x, y)):\n",
    "            x_s = np.shape(x)\n",
    "            dm = np.zeros((x_s[0] * (x_s[0] - 1)) // 2, dtype=np.double)\n",
    "            \n",
    "            p = ProgressBar(shape(dm)[0])\n",
    "            \n",
    "            for i in range(0, x_s[0] - 1):\n",
    "                for j in range(i + 1, x_s[0]):\n",
    "                    dm[dm_count] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                      y[j, ::self.subsample_step])\n",
    "                    \n",
    "                    dm_count += 1\n",
    "                    p.animate(dm_count)\n",
    "            \n",
    "            # Convert to squareform\n",
    "            dm = squareform(dm)\n",
    "            return dm\n",
    "        \n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        else:\n",
    "            x_s = np.shape(x)\n",
    "            y_s = np.shape(y)\n",
    "            dm = np.zeros((x_s[0], y_s[0])) \n",
    "            dm_size = x_s[0]*y_s[0]\n",
    "            \n",
    "            p = ProgressBar(dm_size)\n",
    "        \n",
    "            for i in range(0, x_s[0]):\n",
    "                for j in range(0, y_s[0]):\n",
    "                    dm[i, j] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                  y[j, ::self.subsample_step])\n",
    "                    # Update progress bar\n",
    "                    dm_count += 1\n",
    "                    p.animate(dm_count)\n",
    "        \n",
    "            return dm\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the class labels or probability estimates for \n",
    "        the provided data\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "          x : array of shape [n_samples, n_timepoints]\n",
    "              Array containing the testing data set to be classified\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "          2 arrays representing:\n",
    "              (1) the predicted class labels \n",
    "              (2) the knn label count probability\n",
    "        \"\"\"\n",
    "        \n",
    "        dm = self._dist_matrix(x, self.x)\n",
    "\n",
    "        # Identify the k nearest neighbors\n",
    "        knn_idx = dm.argsort()[:, :self.n_neighbors]\n",
    "\n",
    "        # Identify k nearest labels\n",
    "        knn_labels = self.l[knn_idx]\n",
    "        \n",
    "        # Model Label\n",
    "        mode_data = mode(knn_labels, axis=1)\n",
    "        mode_label = mode_data[0]\n",
    "        mode_proba = mode_data[1]/self.n_neighbors\n",
    "\n",
    "        return mode_label.ravel(), mode_proba.ravel()\n",
    "\n",
    "class ProgressBar:\n",
    "    \"\"\"This progress bar was taken from PYMC\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 40\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        print ('\\r', self,)\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KnnDtw(n_neighbors=1, max_warping_window=10)\n",
    "m.fit(X_train[::10], y_train[::10])\n",
    "label = m.predict(X_test[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(label, y_test[::10],target_names=[l for l in labels.values()]))\n",
    "\n",
    "conf_mat = confusion_matrix(label, y_test[::10])\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "width = np.shape(conf_mat)[1]\n",
    "height = np.shape(conf_mat)[0]\n",
    "\n",
    "res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\n",
    "for i, row in enumerate(conf_mat):\n",
    "    for j, c in enumerate(row):\n",
    "        if c>0:\n",
    "            plt.text(j-.2, i+.1, c, fontsize=16)\n",
    "            \n",
    "cb = fig.colorbar(res)\n",
    "plt.title('Confusion Matrix')\n",
    "_ = plt.xticks(range(6), [l for l in labels.values()], rotation=90)\n",
    "_ = plt.yticks(range(6), [l for l in labels.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test[::10])\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test[::10], axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
